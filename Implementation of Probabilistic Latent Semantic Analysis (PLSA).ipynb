{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXApkhu06tvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa190d75-440a-4cce-8d2b-766542474648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 749 kB 39.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install plsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSSkt_HS5UQw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys \n",
        "import plsa \n",
        "import re, unicodedata \n",
        "import nltk \n",
        "import inflect \n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer \n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "#nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpMiFi5v8hOM"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(words):\n",
        "    'Remove non-ASCII characters from list of tokenized words'\n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "         new_words.append(new_word) \n",
        "    return new_words\n",
        "\n",
        "\n",
        "def to_lowercase (words):\n",
        "    'Convert all characters to lowercase from list of tokenized words' \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "         new_word = word.lower()\n",
        "         new_words.append(new_word) \n",
        "    return new_words\n",
        "\n",
        "\n",
        "def remove_punctuation(words):\n",
        "  'Remove punctuation from list of tokenized words' \n",
        "  new_words = [] \n",
        "  for word in words:\n",
        "      new_word = re.sub(r'[^\\w\\s]', '', word) \n",
        "      if new_word != '':\n",
        "         new_words.append(new_word) \n",
        "  return new_words\n",
        "\n",
        "\n",
        "def replace_numbers(words):\n",
        "   'Replace all interger occurrences in list of tokenized words with textual representation'\n",
        "   p = inflect.engine() \n",
        "   new_words = [] \n",
        "   for word in words: \n",
        "      if word.isdigit():\n",
        "         new_word = p.number_to_words(word)\n",
        "         new_words.append(new_word) \n",
        "      else:\n",
        "            new_words.append(word)  \n",
        "   return new_words\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        " 'Remove stop words from list of tokenized words'\n",
        " new_words = [] \n",
        " for word in words: \n",
        "     if word not in stopwords.words('english'):\n",
        "        new_words.append(word) \n",
        " return new_words\n",
        "\n",
        "  \n",
        "def stem_words(words):\n",
        "   'Stem words in list of tokenized words' \n",
        "   stemmer = LancasterStemmer() \n",
        "   stems = [] \n",
        "   for word in words:\n",
        "     stem = stemmer.stem(word)\n",
        "     stems.append(stem) \n",
        "   return stems\n",
        "\n",
        "\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "   'Lemmatize verbs in list of tokenized words' \n",
        "   lemmatizer = WordNetLemmatizer() \n",
        "   lemmas = [] \n",
        "   for word in words:\n",
        "       lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "       lemmas.append(lemma) \n",
        "   return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words) \n",
        "    words = to_lowercase(words) \n",
        "    words = remove_punctuation(words) \n",
        "    words = replace_numbers(words) \n",
        "    words = remove_stopwords(words) \n",
        "    return words\n",
        "\n",
        "\n",
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words) \n",
        "    lemmas = lemmatize_verbs(words) \n",
        "    return stems, lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To read  pickle_file\n",
        "data_file = pd.read_pickle(r\"database20190924_parsed.pickle\")\n",
        "data_file[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUUTMgb55KUt",
        "outputId": "1cdd731d-243b-45bd-df84-55ba07ada492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Machine learning for discovering missing or wrong protein function annotations',\n",
              "  ['Felipe Kenji Nakano', 'Mathias Lietaert', 'Celine Vens'],\n",
              "  'Hierarchical multi-label classification;Protein function prediction;Benchmark datasets',\n",
              "  'Machine Learning and Artificial Intelligence in Bioinformatics',\n",
              "  'A massive amount of proteomic data is generated on a daily basis, nonetheless annotating all sequences is costly and often unfeasible. As a countermeasure, machine learning methods have been used to automatically annotate new protein functions. More specifically, many studies have investigated hierarchical multi-label classification (HMC) methods to predict annotations, using the Functional Catalogue (FunCat) or Gene Ontology (GO) label hierarchies. Most of these studies employed benchmark datasets created more than a decade ago, and thus train their models on outdated information. In this work, we provide an updated version of these datasets. By querying recent versions of FunCat and GO yeast annotations, we provide 24 new datasets in total. We compare four HMC methods, providing baseline results for the new datasets. Furthermore, we also evaluate whether the predictive models are able to discover new or wrong annotations, by training them on the old data and evaluating their results against the most recent information. The results demonstrated that the method based on predictive clustering trees, Clus-Ensemble, proposed in 2008, achieved superior results compared to more recent methods on the standard evaluation task. For the discovery of new knowledge, Clus-Ensemble performed better when discovering new annotations in the FunCat taxonomy, whereas hierarchical multi-label classification with genetic algorithm (HMC-GA), a method based on genetic algorithms, was overall superior when detecting annotations that were removed. In the GO datasets, Clus-Ensemble once again had the upper hand when discovering new annotations, HMC-GA performed better for detecting removed annotations. However, in this evaluation, there were less significant differences among the methods. The experiments have showed that protein function prediction is a very challenging task which should be further investigated. We believe that the baseline results associated with the updated datasets provided in this work should be considered as guidelines for future studies, nonetheless the old versions of the datasets should not be disregarded since other tasks in machine learning could benefit from them.'),\n",
              " ('Automatic discovery of 100-miRNA signature for cancer classification using ensemble feature selection',\n",
              "  ['Alejandro Lopez-Rincon',\n",
              "   'Marlet Martinez-Archundia',\n",
              "   'Gustavo U. Martinez-Ruiz',\n",
              "   'Alexander Schoenhuth',\n",
              "   'Alberto Tonda'],\n",
              "  'MicroRNAs;miRNA;Feature selection;Machine learning;Classifiers;Dataset',\n",
              "  'Machine Learning and Artificial Intelligence in Bioinformatics',\n",
              "  'MicroRNAs (miRNAs) are noncoding RNA molecules heavily involved in human tumors, in which few of them circulating the human body. Finding a tumor-associated signature of miRNA, that is, the minimum miRNA entities to be measured for discriminating both different types of cancer and normal tissues, is of utmost importance. Feature selection techniques applied in machine learning can help however they often provide naive or biased results. An ensemble feature selection strategy for miRNA signatures is proposed. miRNAs are chosen based on consensus on feature relevance from high-accuracy classifiers of different typologies. This methodology aims to identify signatures that are considerably more robust and reliable when used in clinically relevant prediction tasks. Using the proposed method, a 100-miRNA signature is identified in a dataset of 8023 samples, extracted from TCGA. When running eight-state-of-the-art classifiers along with the 100-miRNA signature against the original 1046 features, it could be detected that global accuracy differs only by 1.4%. Importantly, this 100-miRNA signature is sufficient to distinguish between tumor and normal tissues. The approach is then compared against other feature selection methods, such as UFS, RFE, EN, LASSO, Genetic Algorithms, and EFS-CLA. The proposed approach provides better accuracy when tested on a 10-fold cross-validation with different classifiers and it is applied to several GEO datasets across different platforms with some classifiers showing more than 90% classification accuracy, which proves its cross-platform applicability. The 100-miRNA signature is sufficiently stable to provide almost the same classification accuracy as the complete TCGA dataset, and it is further validated on several GEO datasets, across different types of cancer and platforms. Furthermore, a bibliographic analysis confirms that 77 out of the 100 miRNAs in the signature appear in lists of circulating miRNAs used in cancer studies, in stem-loop or mature-sequence form. The remaining 23 miRNAs offer potentially promising avenues for future research.')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### PLAN FOR DATA PREPARATION\n",
        "# 1. Scan to Find all unique words in the whole document :  These will be column names\n",
        "# 1.1 Initialize to zero for all cells, matrices\n",
        "# 1.2 Count all words for each column in each article (row), update the initially initialized counts in cell from step 1.1\n",
        "\n",
        "\n",
        "# 2. Remove words with a total count of less than 3\n",
        "# 3. Remove 20% of the most common words???? most common?? how??? what is the threshold?\n",
        "\n",
        "# 4. Continue with PLSA implementation"
      ],
      "metadata": {
        "id": "vZFTl0xP-Wqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = [(title,[authors],keywords,classification,abstract),(),(),()]\n",
        "def preprocessing(data=data_file):\n",
        "    for article in data:\n",
        "        complete_article = article[0] +\" \"+ \" \".join(article[1]) + article[2] + \" \" + article[3] + \" \"+article[4]\n",
        "        words = nltk.word_tokenize(complete_article)\n",
        "        words = normalize(words)\n",
        "        _,lemmas = stem_and_lemmatize(words)\n",
        "        yield lemmas\n",
        "\n",
        "\n",
        "def scan_unique_words(data):\n",
        "    words = []\n",
        "    for article in data:\n",
        "        words.extend(set(article))\n",
        "    return set(words)\n",
        "\n",
        "def data_matrix(data,column_names,threshold):\n",
        "    # initialize an empty matrix\n",
        "    data = list(data)\n",
        "    rows = len(data) # change later\n",
        "    cols = len(column_names)\n",
        "    empty_data = np.zeros((rows,cols))\n",
        "    print(f\"Start Shape: {(rows,cols)}\")\n",
        "    # Count words in each article\n",
        "    for article in range(rows):\n",
        "        clear_output(wait=True) \n",
        "        for key_word in range(cols):\n",
        "            empty_data[article,key_word] += \" \".join(data[article]).count(column_names[key_word])\n",
        "        print(f'{100*article/rows:.0f}% articles processed {article} of {rows}')        \n",
        "    # Generate Pandas df\n",
        "    df = pd.DataFrame(data=empty_data,columns=column_names)\n",
        "    # Final Touch file\n",
        "    clear_output(wait=True) \n",
        "    print(\"final_preprocessed_file data\")\n",
        "    df = final_processed_file(df,threshold)\n",
        "    df.to_csv(\"processed_data.csv\",index=False)\n",
        "    print(\"All done ...\")\n",
        "    print(f\"Start Shape: {(rows,cols)}\")\n",
        "    print(f\"End Shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def final_processed_file(df,threshold):\n",
        "    '''\n",
        "    threshold = (0,1)\n",
        "    '''\n",
        "    # drop colums with less than 3 counts in total\n",
        "    # randomly drop columns with 20% of the total counts exceeding threshold\n",
        "    # Remove some noise, words with less than 3 letters for example\n",
        "    cols = df.columns\n",
        "    i=0\n",
        "    for c in cols:\n",
        "        if len(c) < 3:\n",
        "            df.drop(c,axis=1,inplace=True)\n",
        "            i+=1\n",
        "\n",
        "        \n",
        "\n",
        "    sums = df.sum()\n",
        "    cols = sums[sums<3].index\n",
        "\n",
        "\n",
        "    if list(cols):\n",
        "        print(f\"Dropping columns with less than 3 counts in total.....\\n{list(cols)}\\n\")\n",
        "        df.drop(cols,axis=1,inplace=True)\n",
        "\n",
        "    sums = df.sum()\n",
        "    max_sum = sums.values.max()\n",
        "    exceed_th = sums[sums >= threshold*max_sum].index\n",
        "\n",
        "    if list(exceed_th):\n",
        "        indx = np.random.permutation(int(len(exceed_th)*0.2))\n",
        "        exceed_th_20p = exceed_th[indx]\n",
        "        print(f\"Randomly drop columns with 20% of the total counts exceeding threshold:{threshold*max_sum:.0f}\\n{list(exceed_th_20p)}\\n\") \n",
        "        df.drop(exceed_th_20p,axis=1,inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jlp5mMALCu67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = preprocessing(data_file)\n",
        "preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QXdQVupPQBg",
        "outputId": "480e92ca-0639-4244-e52c-3672f8bd216f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object preprocessing at 0x7f1f9bb67b50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = list(scan_unique_words(preprocessed))"
      ],
      "metadata": {
        "id": "NK7J0oCMSfVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScZK_nPGTLiy",
        "outputId": "0fe4845d-4237-4061-ef92-36acd898f46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41381"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = data_matrix(preprocessing(data_file),unique_words,0.05)"
      ],
      "metadata": {
        "id": "gIEWRofLXYv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "FC-eFq7yyKDK",
        "outputId": "c073cbfe-c37f-4197-9b9c-80746a4a5a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9c24a5c8-3228-4fb8-b1e8-1cbc4fa940e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unknown</th>\n",
              "      <th>plus</th>\n",
              "      <th>jquery</th>\n",
              "      <th>subgraph</th>\n",
              "      <th>minimum</th>\n",
              "      <th>mens</th>\n",
              "      <th>immunotherapy</th>\n",
              "      <th>bpms</th>\n",
              "      <th>linedrug</th>\n",
              "      <th>diurnal</th>\n",
              "      <th>benjamin</th>\n",
              "      <th>wormtable</th>\n",
              "      <th>mirs</th>\n",
              "      <th>exac</th>\n",
              "      <th>japanese</th>\n",
              "      <th>consumer</th>\n",
              "      <th>evfoldmfdca</th>\n",
              "      <th>mutant</th>\n",
              "      <th>iris</th>\n",
              "      <th>barcoding</th>\n",
              "      <th>theor</th>\n",
              "      <th>novice</th>\n",
              "      <th>bioshell</th>\n",
              "      <th>postprocess</th>\n",
              "      <th>unless</th>\n",
              "      <th>ravo</th>\n",
              "      <th>akash</th>\n",
              "      <th>serine</th>\n",
              "      <th>banga</th>\n",
              "      <th>hope</th>\n",
              "      <th>hide</th>\n",
              "      <th>twopart</th>\n",
              "      <th>ethical</th>\n",
              "      <th>syringae</th>\n",
              "      <th>erin</th>\n",
              "      <th>cyril</th>\n",
              "      <th>santella</th>\n",
              "      <th>harper</th>\n",
              "      <th>ams</th>\n",
              "      <th>modern</th>\n",
              "      <th>...</th>\n",
              "      <th>organism</th>\n",
              "      <th>bruce</th>\n",
              "      <th>sspace</th>\n",
              "      <th>liver</th>\n",
              "      <th>hamper</th>\n",
              "      <th>constraintbased</th>\n",
              "      <th>seventy</th>\n",
              "      <th>nine hundred and fifty-eight</th>\n",
              "      <th>joaquin</th>\n",
              "      <th>mya</th>\n",
              "      <th>fpgas</th>\n",
              "      <th>bifurcation</th>\n",
              "      <th>nonspecific</th>\n",
              "      <th>competitive</th>\n",
              "      <th>perhaps</th>\n",
              "      <th>generality</th>\n",
              "      <th>kingdom</th>\n",
              "      <th>circular</th>\n",
              "      <th>hcc</th>\n",
              "      <th>kefed</th>\n",
              "      <th>supercoiling</th>\n",
              "      <th>four</th>\n",
              "      <th>lad</th>\n",
              "      <th>woods</th>\n",
              "      <th>terminus</th>\n",
              "      <th>gapfill</th>\n",
              "      <th>efficiency</th>\n",
              "      <th>anticancer</th>\n",
              "      <th>lymphocytic</th>\n",
              "      <th>rnac</th>\n",
              "      <th>variate</th>\n",
              "      <th>georeferenced</th>\n",
              "      <th>alan</th>\n",
              "      <th>yee</th>\n",
              "      <th>gtex</th>\n",
              "      <th>algorithmbayesian</th>\n",
              "      <th>metallopeptidaselike</th>\n",
              "      <th>beanalyzer</th>\n",
              "      <th>mone</th>\n",
              "      <th>establishment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 16422 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c24a5c8-3228-4fb8-b1e8-1cbc4fa940e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c24a5c8-3228-4fb8-b1e8-1cbc4fa940e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c24a5c8-3228-4fb8-b1e8-1cbc4fa940e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   unknown  plus  jquery  ...  beanalyzer  mone  establishment\n",
              "0      0.0   0.0     0.0  ...         0.0   0.0            0.0\n",
              "1      0.0   0.0     0.0  ...         0.0   0.0            0.0\n",
              "2      0.0   0.0     0.0  ...         0.0   0.0            0.0\n",
              "3      0.0   0.0     0.0  ...         0.0   0.0            0.0\n",
              "4      0.0   0.0     0.0  ...         0.0   0.0            0.0\n",
              "\n",
              "[5 rows x 16422 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/processed_data.csv\")"
      ],
      "metadata": {
        "id": "h1C32odmq2hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irl1NnAP08N2"
      },
      "source": [
        "py2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBVS_G8V12cf"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from sklearn.preprocessing import normalize\n",
        "import sys\n",
        "\n",
        "class Plsa(object):\n",
        "      def __init__(self, dataset):\n",
        "          self.data_set = dataset.values #data set \n",
        "          self.size_articles = 10 #number of articles taken \n",
        "          self.k = 10  #number of topics, by default 18 \n",
        "          self.size_M = self.data_set.size\n",
        "\n",
        "          \n",
        "          '''We have try to have the same naming with the formula that is going to be implemented \n",
        "             So we make the naming similar to the formulas.'''\n",
        "           # probabilities \n",
        "          self.D_W = [] #P ( d| w) \n",
        "          self.Z_D = [] # P(Z / d) \n",
        "          self.Z_D_W = [] # P(z / d,w) \n",
        "          self.W_Z = [] # P(w /z) \n",
        "          self.P_D = [] # P(d ) \n",
        "          self.W_D = [] # P(w/d)\n",
        "\n",
        "      def ini_matrix_prob(self): \n",
        "          #initialize empty matrices and also generate matrices\n",
        "          self.Z_D = np.zeros([self.k, self.size_articles], dtype = np.float32) \n",
        "          self.W_Z = np.zeros([self.size_M, self.k], dtype= np.float32) \n",
        "          self.P_D = np.zeros((self.size_articles), dtype=np.float32) \n",
        "          self.Z_D_W = np.zeros([self.k, self.size_articles, self.size_M], dtype=np.float32) \n",
        "          self.W_D = np.zeros([self.size_M, self.size_articles], dtype=np.float32) \n",
        "          self.D_W = np.zeros([self.size_articles, self.size_M], dtype=np.float32)\n",
        "\n",
        "          #assign randon values \n",
        "          self.Z_D =np.random.random(size=(self.k, self.size_articles)) \n",
        "          self.W_Z = np.random.random(size=(self.size_M, self.k))\n",
        "\n",
        "          #probability \n",
        "          p_d= 1.0 / self.size_articles \n",
        "          #  for i in range(self.size_articles):\n",
        "          #      self.P_D[i] = p_d\n",
        "          self.P_D = np.full(self.size_articles,p_d,dtype=np.float32)\n",
        "\n",
        "      def Log_likelihood(self, iteration): # method of calculating the log-likelihood\n",
        "         Loglik = 0.0 \n",
        "         for d in range(self.size_articles): \n",
        "            for w in range(self.size_M):\n",
        "                for z in range(self.k):\n",
        "                    Loglik = Loglik + self.D_W[d, w]*(np.log(self.Z_D[z,d] * self.W_Z[w, z]) \n",
        "                    if self.Z_D[z, d]* self.W_Z[w, z] != 0 else 0)\n",
        "\n",
        "         print('ITERATION #', iteration,' :') \n",
        "         print('New Log likelihood:', Loglik)\n",
        "\n",
        "      def Probabilities(self): # method to calculates probabilities (w/ d) and P(d / w) \n",
        "          for d in range(self.size_articles): \n",
        "              for w in range(self.size_M):\n",
        "                  self.W_D[w, d] = np.sum(self.W_Z[w, :] *self.Z_D[:, d]) \n",
        "              normalize(self.W_D, norm='l1', axis=0, copy=False) #normalization \n",
        "              for w in range(self.size_M):\n",
        "                  self.D_W[:, w] = self.P_D[:] * self.W_D[w, :]\n",
        "\n",
        "              normalize(self.D_W, norm= 'l1', axis=0, copy=False) #normalization\n",
        "              \n",
        "\n",
        "      def E_step(self): #method of Estep \n",
        "         for w in range(self.size_M): \n",
        "              for d in range(self.size_articles):\n",
        "                  self.Z_D_W[:, d, w] = self.W_Z[w,:] * self.Z_D[:, d] \n",
        "                  normalize(self.Z_D_W[:, :, w], norm='l1', axis=0 , copy=False) # normalization\n",
        "\n",
        "      def M_step(self): # method of H step\n",
        "          self.W_Z = np.zeros([self.size_M, self.k], dtype=np.float)# P(z/ d) \n",
        "          for z in range(self.k): \n",
        "            for w in range(self.size_M):\n",
        "                self.W_Z[w, z] = np.sum(self.data_set[z][:self.k] *self.Z_D_W[z, :, w])\n",
        "                #normalize(self.W_Z, norm='l1', axis=0, copy=False) # normalization\n",
        "         \n",
        "          for d in range(self.size_articles): # update P(z I d) \n",
        "              for z in range(self.k):\n",
        "                   self.Z_D[z][d] = np.sum(self.Z_D_W[z, d, :self.data_set.shape[1]] *self.data_set[d][:]) \n",
        "                   self.Z_D[z][d] = self.Z_D[z][d] / np.sum(self.data_set[d])\\\n",
        "                        if np.sum(self.data_set[d]) != 0 \\\n",
        "                         else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(argv):\n",
        "     plsa_cal = Plsa(df2[df2.columns[:500]][:200])\n",
        "     plsa_cal.ini_matrix_prob()\n",
        "     for i in range(6):\n",
        "         plsa_cal.E_step()\n",
        "         plsa_cal.M_step() \n",
        "         plsa_cal.Probabilities() \n",
        "         plsa_cal.Log_likelihood(i)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(sys.argv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oG5sqBuN65J",
        "outputId": "80e89689-0508-49a9-fdcc-c3f08c49084a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITERATION # 0  :\n",
            "New Log likelihood: -1553168.1866551286\n",
            "ITERATION # 1  :\n",
            "New Log likelihood: -903019.0699608877\n",
            "ITERATION # 2  :\n",
            "New Log likelihood: -1418368.9453970077\n",
            "ITERATION # 3  :\n",
            "New Log likelihood: -2442773.7445479087\n",
            "ITERATION # 4  :\n",
            "New Log likelihood: -4502577.125588146\n",
            "ITERATION # 5  :\n",
            "New Log likelihood: -8783554.44859244\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Mwaimu_Sosna_MWHD_LAB3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}